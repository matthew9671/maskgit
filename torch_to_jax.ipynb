{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4358aa57-c0bf-4783-aabc-dc108232511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90026bba-5305-4e2d-9bf6-b9db696920c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From VectorQuantizer2 in modules/vqvae/quantize.py\n",
    "class VectorQuantizer(nn.Module):\n",
    "    n_e: int\n",
    "    e_dim: int\n",
    "    beta: float\n",
    "    remap: str = None\n",
    "    unknown_index: str = \"random\"\n",
    "    sane_index_shape: bool = False\n",
    "    legacy: bool = True\n",
    "\n",
    "    def setup(self):\n",
    "        self.embedding = self.param('embedding', nn.initializers.uniform(scale=1.0 / self.n_e), (self.n_e, self.e_dim))\n",
    "        if self.remap is not None:\n",
    "            self.used = self.param('used', lambda rng, shape: jnp.array(np.load(self.remap)), (self.n_e,))\n",
    "            self.re_embed = self.used.shape[0]\n",
    "            if self.unknown_index == \"extra\":\n",
    "                self.unknown_index = self.re_embed\n",
    "                self.re_embed += 1\n",
    "            print(f\"Remapping {self.n_e} indices to {self.re_embed} indices. \"\n",
    "                  f\"Using {self.unknown_index} for unknown indices.\")\n",
    "        else:\n",
    "            self.re_embed = self.n_e\n",
    "\n",
    "    def remap_to_used(self, inds):\n",
    "        ishape = inds.shape\n",
    "        assert len(ishape) > 1\n",
    "        inds = inds.reshape(ishape[0], -1)\n",
    "        used = self.used\n",
    "        match = (inds[:, :, None] == used[None, None, ...]).astype(jnp.int32)\n",
    "        new = jnp.argmax(match, axis=-1)\n",
    "        unknown = jnp.sum(match, axis=-1) < 1\n",
    "        if self.unknown_index == \"random\":\n",
    "            new = jnp.where(unknown, jax.random.randint(jax.random.PRNGKey(0), new.shape, 0, self.re_embed), new)\n",
    "        else:\n",
    "            new = jnp.where(unknown, self.unknown_index, new)\n",
    "        return new.reshape(ishape)\n",
    "\n",
    "    def unmap_to_all(self, inds):\n",
    "        ishape = inds.shape\n",
    "        assert len(ishape) > 1\n",
    "        inds = inds.reshape(ishape[0], -1)\n",
    "        used = self.used\n",
    "        if self.re_embed > used.shape[0]:  # extra token\n",
    "            inds = jnp.where(inds >= used.shape[0], 0, inds)  # simply set to zero\n",
    "        back = used[inds]\n",
    "        return back.reshape(ishape)\n",
    "\n",
    "    def __call__(self, z, temp=None, rescale_logits=False, return_logits=False):\n",
    "        assert temp is None or temp == 1.0, \"Only for interface compatible with Gumbel\"\n",
    "        assert rescale_logits == False, \"Only for interface compatible with Gumbel\"\n",
    "        assert return_logits == False, \"Only for interface compatible with Gumbel\"\n",
    "        z = jnp.transpose(z, (0, 2, 3, 1))\n",
    "        z_flattened = jnp.reshape(z, [-1, self.e_dim])\n",
    "\n",
    "        d = (jnp.sum(z_flattened ** 2, axis=1, keepdims=True) +\n",
    "             jnp.sum(self.embedding ** 2, axis=1) -\n",
    "             2 * jnp.dot(z_flattened, self.embedding.T))\n",
    "\n",
    "        min_encoding_indices = jnp.argmin(d, axis=1)\n",
    "        z_q = self.embedding[min_encoding_indices]\n",
    "        z_q = jnp.reshape(z_q, z.shape)\n",
    "\n",
    "        if not self.legacy:\n",
    "            loss = self.beta * jnp.mean((jax.lax.stop_gradient(z_q) - z) ** 2) + \\\n",
    "                   jnp.mean((z_q - jax.lax.stop_gradient(z)) ** 2)\n",
    "        else:\n",
    "            loss = jnp.mean((jax.lax.stop_gradient(z_q) - z) ** 2) + \\\n",
    "                   self.beta * jnp.mean((z_q - jax.lax.stop_gradient(z)) ** 2)\n",
    "\n",
    "        z_q = z + jax.lax.stop_gradient(z_q - z)\n",
    "\n",
    "        z_q = jnp.transpose(z_q, (0, 3, 1, 2))\n",
    "\n",
    "        if self.remap is not None:\n",
    "            min_encoding_indices = min_encoding_indices.reshape(z.shape[0], -1)\n",
    "            min_encoding_indices = self.remap_to_used(min_encoding_indices)\n",
    "            min_encoding_indices = min_encoding_indices.reshape(-1)\n",
    "\n",
    "        if self.sane_index_shape:\n",
    "            min_encoding_indices = min_encoding_indices.reshape(z_q.shape[0], z_q.shape[2], z_q.shape[3])\n",
    "\n",
    "        return z_q, loss, (None, None, min_encoding_indices)\n",
    "\n",
    "    def get_codebook_entry(self, indices, shape):\n",
    "        if self.remap is not None:\n",
    "            indices = indices.reshape(shape[0], -1)\n",
    "            indices = self.unmap_to_all(indices)\n",
    "            indices = indices.reshape(-1)\n",
    "\n",
    "        z_q = self.embedding[indices]\n",
    "\n",
    "        if shape is not None:\n",
    "            z_q = z_q.reshape(shape)\n",
    "            z_q = jnp.transpose(z_q, (0, 3, 1, 2))\n",
    "\n",
    "        return z_q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cb5a3fd-3065-4583-b967-2ed5f3637387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestep_embedding(timesteps, embedding_dim):\n",
    "    \"\"\"\n",
    "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n",
    "    From Fairseq.\n",
    "    Build sinusoidal embeddings.\n",
    "    This matches the implementation in tensor2tensor, but differs slightly\n",
    "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "    \"\"\"\n",
    "    assert len(timesteps.shape) == 1\n",
    "\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = jnp.log(10000) / (half_dim - 1)\n",
    "    emb = jnp.exp(jnp.arange(half_dim) * -emb)\n",
    "    emb = timesteps[:, None] * emb[None, :]\n",
    "    emb = jnp.concatenate([jnp.sin(emb), jnp.cos(emb)], axis=1)\n",
    "    if embedding_dim % 2 == 1:  # zero pad\n",
    "        emb = jnp.pad(emb, ((0, 0), (0, 1)))\n",
    "    return emb\n",
    "\n",
    "def nonlinearity(x):\n",
    "    # swish\n",
    "    return x * nn.sigmoid(x)\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    # in_channels: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        return nn.GroupNorm(num_groups=32, epsilon=1e-6, use_scale=True, use_bias=True)(x)\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    in_channels: int\n",
    "    with_conv: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = jax.image.resize(x, (x.shape[0], x.shape[1] * 2, x.shape[2] * 2, x.shape[3]), method='nearest')\n",
    "        if self.with_conv:\n",
    "            x = nn.Conv(self.in_channels, kernel_size=(3, 3), strides=(1, 1), padding='SAME')(x)\n",
    "        return x\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    in_channels: int\n",
    "    with_conv: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.with_conv:\n",
    "            x = nn.Conv(self.in_channels, kernel_size=(3, 3), strides=(2, 2), padding='SAME')(x)\n",
    "        else:\n",
    "            x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2), padding='VALID')\n",
    "        return x\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    in_channels: int\n",
    "    out_channels: int = None\n",
    "    conv_shortcut: bool = False\n",
    "    dropout: float = 0.0\n",
    "    temb_channels: int = 512\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, temb, deterministic):\n",
    "        out_channels = self.in_channels if self.out_channels is None else self.out_channels\n",
    "\n",
    "        h = Normalize()(x)\n",
    "        h = nonlinearity(h)\n",
    "        h = nn.Conv(out_channels, kernel_size=(3, 3), strides=(1, 1), padding='SAME')(h)\n",
    "\n",
    "        if self.temb_channels > 0 and temb is not None:\n",
    "            h = h + nn.Dense(out_channels)(nonlinearity(temb))[:, None, None, :]\n",
    "\n",
    "        h = Normalize()(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = nn.Dropout(rate=self.dropout, deterministic=deterministic)(h)\n",
    "        h = nn.Conv(out_channels, kernel_size=(3, 3), strides=(1, 1), padding='SAME')(h)\n",
    "\n",
    "        if self.in_channels != out_channels:\n",
    "            if self.conv_shortcut:\n",
    "                x = nn.Conv(out_channels, kernel_size=(3, 3), strides=(1, 1), padding='SAME')(x)\n",
    "            else:\n",
    "                x = nn.Conv(out_channels, kernel_size=(1, 1), strides=(1, 1), padding='SAME')(x)\n",
    "\n",
    "        return x + h\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    in_channels: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        h_ = Normalize()(x)\n",
    "        q = nn.Conv(self.in_channels, kernel_size=(1, 1))(h_)\n",
    "        k = nn.Conv(self.in_channels, kernel_size=(1, 1))(h_)\n",
    "        v = nn.Conv(self.in_channels, kernel_size=(1, 1))(h_)\n",
    "\n",
    "        b, c, h, w = q.shape\n",
    "        q = jnp.reshape(q, (b, c, h * w))\n",
    "        q = jnp.transpose(q, (0, 2, 1))\n",
    "        k = jnp.reshape(k, (b, c, h * w))\n",
    "        w_ = jnp.matmul(q, k) * (int(c) ** (-0.5))\n",
    "        w_ = nn.softmax(w_, axis=2)\n",
    "\n",
    "        v = jnp.reshape(v, (b, c, h * w))\n",
    "        w_ = jnp.transpose(w_, (0, 2, 1))\n",
    "        h_ = jnp.matmul(v, w_)\n",
    "        h_ = jnp.reshape(h_, (b, c, h, w))\n",
    "\n",
    "        h_ = nn.Conv(self.in_channels, kernel_size=(1, 1))(h_)\n",
    "\n",
    "        return x + h_\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ch: int\n",
    "    out_ch: int\n",
    "    ch_mult: tuple = (1, 2, 4, 8)\n",
    "    num_res_blocks: int = 2\n",
    "    attn_resolutions: tuple = (16,)\n",
    "    dropout: float = 0.0\n",
    "    resamp_with_conv: bool = True\n",
    "    in_channels: int = 3 # not actually used \n",
    "    resolution: int = 256\n",
    "    z_channels: int = 256\n",
    "    double_z: bool = True\n",
    "\n",
    "    def setup(self):\n",
    "        self.conv1 = nn.Conv(self.ch, kernel_size=(3, 3), strides=(1, 1), padding='SAME')\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, deterministic):\n",
    "        temb = None\n",
    "        # hs = [nn.Conv(self.ch, kernel_size=(3, 3), strides=(1, 1), padding='SAME')(x)]\n",
    "        hs = [self.conv1(x)]\n",
    "        curr_res = self.resolution\n",
    "\n",
    "        for i_level in range(len(self.ch_mult)):\n",
    "            for i_block in range(self.num_res_blocks):\n",
    "                h = ResnetBlock(in_channels=hs[-1].shape[-1], out_channels=self.ch * self.ch_mult[i_level], \n",
    "                                temb_channels=0, dropout=self.dropout)(hs[-1], temb, deterministic)\n",
    "                if curr_res in self.attn_resolutions:\n",
    "                    h = AttnBlock(in_channels=h.shape[-1])(h)\n",
    "                hs.append(h)\n",
    "            if i_level != len(self.ch_mult) - 1:\n",
    "                hs.append(Downsample(in_channels=h.shape[-1], with_conv=self.resamp_with_conv)(hs[-1]))\n",
    "                curr_res //= 2\n",
    "\n",
    "        h = ResnetBlock(in_channels=hs[-1].shape[-1], out_channels=hs[-1].shape[-1], \n",
    "                        temb_channels=0, dropout=self.dropout)(hs[-1], temb, deterministic)\n",
    "        h = AttnBlock(in_channels=h.shape[-1])(h)\n",
    "        h = ResnetBlock(in_channels=h.shape[-1], out_channels=hs[-1].shape[-1], \n",
    "                        temb_channels=0, dropout=self.dropout)(h, temb, deterministic)\n",
    "\n",
    "        h = Normalize()(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = nn.Conv(2 * self.z_channels if self.double_z else self.z_channels, \n",
    "                    kernel_size=(3, 3), strides=(1, 1), padding='SAME')(h)\n",
    "        return h\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    ch: int\n",
    "    out_ch: int\n",
    "    ch_mult: tuple = (1, 2, 4, 8)\n",
    "    num_res_blocks: int = 2\n",
    "    attn_resolutions: tuple = (16,)\n",
    "    dropout: float = 0.0\n",
    "    resamp_with_conv: bool = True\n",
    "    in_channels: int = 3 # not actually used\n",
    "    resolution: int = 256\n",
    "    z_channels: int = 256\n",
    "    give_pre_end: bool = False\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, z, deterministic):\n",
    "        temb = None\n",
    "        block_in = self.ch * self.ch_mult[-1]\n",
    "        curr_res = self.resolution // 2 ** (len(self.ch_mult) - 1)\n",
    "        h = nn.Conv(block_in, kernel_size=(3, 3), strides=(1, 1), padding='SAME')(z)\n",
    "\n",
    "        h = ResnetBlock(in_channels=block_in, out_channels=block_in, \n",
    "                        temb_channels=0, dropout=self.dropout)(h, temb, deterministic)\n",
    "        h = AttnBlock(in_channels=block_in)(h)\n",
    "        h = ResnetBlock(in_channels=block_in, out_channels=block_in, \n",
    "                        temb_channels=0, dropout=self.dropout)(h, temb, deterministic)\n",
    "\n",
    "        for i_level in reversed(range(len(self.ch_mult))):\n",
    "            for i_block in range(self.num_res_blocks + 1):\n",
    "                h = ResnetBlock(in_channels=h.shape[-1], out_channels=self.ch * self.ch_mult[i_level], \n",
    "                                temb_channels=0, dropout=self.dropout)(h, temb, deterministic)\n",
    "                if curr_res in self.attn_resolutions:\n",
    "                    h = AttnBlock(in_channels=h.shape[-1])(h)\n",
    "            if i_level != 0:\n",
    "                h = Upsample(in_channels=h.shape[-1], with_conv=self.resamp_with_conv)(h)\n",
    "                curr_res *= 2\n",
    "\n",
    "        if self.give_pre_end:\n",
    "            return h\n",
    "\n",
    "        h = Normalize()(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = nn.Conv(self.out_ch, kernel_size=(3, 3), strides=(1, 1), padding='SAME')(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d1aa75f-afeb-441d-988a-3dc34456aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQModel(nn.Module):\n",
    "    ddconfig: dict\n",
    "    lossconfig: dict\n",
    "    n_embed: int\n",
    "    embed_dim: int\n",
    "    ckpt_path: str = None\n",
    "    ignore_keys: list = None\n",
    "    image_key: str = \"image\"\n",
    "    colorize_nlabels: int = None\n",
    "    monitor: str = None\n",
    "    remap: None = None\n",
    "    sane_index_shape: bool = False\n",
    "\n",
    "    def setup(self):\n",
    "        self.encoder = Encoder(**self.ddconfig)\n",
    "        self.decoder = Decoder(**self.ddconfig)\n",
    "        self.quantize = VectorQuantizer(n_e=self.n_embed, e_dim=self.embed_dim, \n",
    "                                        beta=0.25, remap=self.remap, sane_index_shape=self.sane_index_shape)\n",
    "        self.quant_conv = nn.Conv(self.embed_dim, (1, 1))\n",
    "        self.post_quant_conv = nn.Conv(self.ddconfig[\"z_channels\"], (1, 1))\n",
    "\n",
    "        if self.colorize_nlabels is not None:\n",
    "            self.colorize = self.param('colorize', nn.initializers.normal(), (3, self.colorize_nlabels, 1, 1))\n",
    "\n",
    "    def encode(self, x, train):\n",
    "        h = self.encoder(x, deterministic=not train)\n",
    "        h = self.quant_conv(h)\n",
    "        quant, emb_loss, info = self.quantize(h)\n",
    "        return quant, emb_loss, info\n",
    "\n",
    "    def decode(self, quant, train):\n",
    "        quant = self.post_quant_conv(quant)\n",
    "        dec = self.decoder(quant, deterministic=not train)\n",
    "        return dec\n",
    "\n",
    "    def decode_code(self, code_b, train):\n",
    "        # TODO: is the shape here correct?\n",
    "        quant_b = self.quantize.get_codebook_entry(code_b.reshape(-1), (-1, code_b.shape[1], code_b.shape[2], self.embed_dim))\n",
    "        dec = self.decode(quant_b, train)\n",
    "        return dec\n",
    "\n",
    "    def __call__(self, input, train):\n",
    "        quant, diff, _ = self.encode(input, train)\n",
    "        dec = self.decode(quant, train)\n",
    "        return dec, diff\n",
    "\n",
    "    def get_input(self, batch, k):\n",
    "        x = batch[k]\n",
    "        if len(x.shape) == 3:\n",
    "            x = jnp.expand_dims(x, axis=-1)\n",
    "        x = jnp.transpose(x, (0, 3, 1, 2))\n",
    "        return x.astype(jnp.float32)\n",
    "\n",
    "    def training_step(self, state, batch, optimizer_idx):\n",
    "        x = self.get_input(batch, self.image_key)\n",
    "        xrec, qloss = self(x, train=True)\n",
    "\n",
    "        def loss_fn(params):\n",
    "            if optimizer_idx == 0:\n",
    "                aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, state.step, \n",
    "                                                last_layer=self.get_last_layer(), split=\"train\")\n",
    "                return aeloss\n",
    "\n",
    "            if optimizer_idx == 1:\n",
    "                discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, state.step, \n",
    "                                                    last_layer=self.get_last_layer(), split=\"train\")\n",
    "                return discloss\n",
    "\n",
    "        grad_fn = jax.value_and_grad(loss_fn)\n",
    "        loss, grads = grad_fn(state.params)\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "        return state, loss\n",
    "\n",
    "    def validation_step(self, state, batch):\n",
    "        x = self.get_input(batch, self.image_key)\n",
    "        xrec, qloss = self(x)\n",
    "        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0, state.step, \n",
    "                                        last_layer=self.get_last_layer(), split=\"val\")\n",
    "        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1, state.step, \n",
    "                                            last_layer=self.get_last_layer(), split=\"val\")\n",
    "        rec_loss = log_dict_ae[\"val/rec_loss\"]\n",
    "        logs = {\"val/rec_loss\": rec_loss, \"val/aeloss\": aeloss}\n",
    "        logs.update(log_dict_ae)\n",
    "        logs.update(log_dict_disc)\n",
    "        return logs\n",
    "\n",
    "    def configure_optimizers(self, lr):\n",
    "        optimizer = optax.adam(learning_rate=lr, b1=0.5, b2=0.9)\n",
    "        return optimizer\n",
    "\n",
    "    def get_last_layer(self):\n",
    "        return self.decoder.layers[-1].kernel\n",
    "\n",
    "    def log_images(self, batch):\n",
    "        x = self.get_input(batch, self.image_key)\n",
    "        xrec, _ = self(x)\n",
    "        if x.shape[-1] > 3:\n",
    "            x = self.to_rgb(x)\n",
    "            xrec = self.to_rgb(xrec)\n",
    "        return {\"inputs\": x, \"reconstructions\": xrec}\n",
    "\n",
    "    def to_rgb(self, x):\n",
    "        assert self.image_key == \"segmentation\"\n",
    "        x = jax.lax.conv_general_dilated(x, self.colorize, window_strides=(1, 1), padding='SAME')\n",
    "        x = 2. * (x - jnp.min(x)) / (jnp.max(x) - jnp.min(x)) - 1.\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64cef4-3946-4ae3-8ae1-d054a212683d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example of how to initialize and use the model\n",
    "ddconfig = {\n",
    "    \"ch\": 128,\n",
    "    \"out_ch\": 3,\n",
    "    \"z_channels\": 256\n",
    "}\n",
    "lossconfig = {}\n",
    "n_embed = 512\n",
    "embed_dim = 64\n",
    "\n",
    "model = VQModel(ddconfig=ddconfig, lossconfig=lossconfig, n_embed=n_embed, embed_dim=embed_dim)\n",
    "\n",
    "# Optimizer and state initialization\n",
    "lr = 1e-4\n",
    "optimizer = model.configure_optimizers(lr)\n",
    "state = train_state.TrainState.create(apply_fn=model.apply, \n",
    "        params=model.init(jax.random.PRNGKey(0), jnp.ones((1, 256, 256, 3)), train=True), tx=optimizer)\n",
    "\n",
    "# Example training step\n",
    "batch = {\"image\": jnp.ones((1, 256, 256, 3))}\n",
    "optimizer_idx = 0\n",
    "state, loss = model.apply(state.params, state, batch, optimizer_idx, method=model.training_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1152a3e9-68f2-4931-8592-8c9e6707ba02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ScopeCollectionNotFound",
     "evalue": "Tried to access \"kernel\" from collection \"params\" in \"/encoder/conv1\" but the collection is empty. (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeCollectionNotFound)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScopeCollectionNotFound\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[86], line 43\u001b[0m, in \u001b[0;36mVQModel.__call__\u001b[0;34m(self, input, train)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, train):\n\u001b[0;32m---> 43\u001b[0m     quant, diff, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     dec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(quant, train)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dec, diff\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[86], line 26\u001b[0m, in \u001b[0;36mVQModel.encode\u001b[0;34m(self, x, train)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, train):\n\u001b[0;32m---> 26\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_conv(h)\n\u001b[1;32m     28\u001b[0m     quant, emb_loss, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantize(h)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[85], line 131\u001b[0m, in \u001b[0;36mEncoder.__call__\u001b[0;34m(self, x, deterministic)\u001b[0m\n\u001b[1;32m    129\u001b[0m temb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# hs = [nn.Conv(self.ch, kernel_size=(3, 3), strides=(1, 1), padding='SAME')(x)]\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m hs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    132\u001b[0m curr_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolution\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_level \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mch_mult)):\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_tpu_py39/lib/python3.9/site-packages/flax/linen/linear.py:636\u001b[0m, in \u001b[0;36m_Conv.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m kernel_shape:\n\u001b[1;32m    631\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMask needs to have the same shape as weights. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShapes are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    634\u001b[0m   )\n\u001b[0;32m--> 636\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkernel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_dtype\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m   kernel \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_tpu_py39/lib/python3.9/site-packages/flax/core/scope.py:988\u001b[0m, in \u001b[0;36mScope.param\u001b[0;34m(self, name, init_fn, unbox, *init_args, **init_kwargs)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_mutable_collection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    987\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_collection_empty(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeCollectionNotFound(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m, name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text)\n\u001b[1;32m    989\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeParamNotFoundError(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text)\n\u001b[1;32m    990\u001b[0m value \u001b[38;5;241m=\u001b[39m init_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_rng(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m*\u001b[39minit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n",
      "\u001b[0;31mScopeCollectionNotFound\u001b[0m: Tried to access \"kernel\" from collection \"params\" in \"/encoder/conv1\" but the collection is empty. (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeCollectionNotFound)"
     ]
    }
   ],
   "source": [
    "model.apply(state.params, batch[\"image\"], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "839ebbfd-7e17-4293-9174-42ec1119a1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/yixiuz/miniconda3/envs/jax_tpu_py39/lib/python3.9/site-packages/flax/core/scope.py\u001b[0m(988)\u001b[0;36mparam\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    986 \u001b[0;31m      \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_mutable_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    987 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_collection_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 988 \u001b[0;31m          \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScopeCollectionNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    989 \u001b[0;31m        \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScopeParamNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    990 \u001b[0;31m      \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.variables\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Scope.variables of <flax.core.scope.Scope object at 0x7fe73a88e6a0>>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.variables()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32205d3b-004c-4395-973e-0ed3620705ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d858efcd-81a2-46ef-919b-7dd33f7dfc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VQModel(nn.Module):\n",
    "    \n",
    "#     encoder: nn.Module = None\n",
    "#     decoder: nn.Module = None\n",
    "#     quantize: nn.Module = None\n",
    "#     quant_conv: nn.Module = None\n",
    "#     post_quant_conv: nn.Module = None\n",
    "#     colorize: None = None\n",
    "    \n",
    "#     # ddconfig: dict\n",
    "#     # lossconfig: dict\n",
    "#     # n_embed: int\n",
    "#     embed_dim: int\n",
    "#     ckpt_path: str = None\n",
    "#     ignore_keys: list = None\n",
    "#     image_key: str = \"image\"\n",
    "#     # colorize_nlabels: int = None\n",
    "#     monitor: str = None\n",
    "#     remap: None = None\n",
    "#     # sane_index_shape: bool = False\n",
    "\n",
    "#     # def setup(self):\n",
    "#     #     self.encoder = Encoder(**self.ddconfig)\n",
    "#     #     self.decoder = Decoder(**self.ddconfig)\n",
    "#     #     self.quantize = VectorQuantizer(n_e=self.n_embed, e_dim=self.embed_dim, \n",
    "#     #                                     beta=0.25, remap=self.remap, sane_index_shape=self.sane_index_shape)\n",
    "#     #     self.quant_conv = nn.Conv(self.embed_dim, (1, 1))\n",
    "#     #     self.post_quant_conv = nn.Conv(self.ddconfig[\"z_channels\"], (1, 1))\n",
    "\n",
    "#     #     if self.colorize_nlabels is not None:\n",
    "#     #         self.colorize = self.param('colorize', nn.initializers.normal(), (3, self.colorize_nlabels, 1, 1))\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_params(cls, ddconfig, lossconfig, n_embed, embed_dim, \n",
    "#         ckpt_path=None, ignore_keys=None, image_key=\"image\", colorize_nlabels=None,\n",
    "#         monitor=None, remap=None, sane_index_shape=False):\n",
    "#         encoder = Encoder(**ddconfig)\n",
    "#         decoder = Decoder(**ddconfig)\n",
    "#         quantize = VectorQuantizer(n_e=n_embed, e_dim=embed_dim, \n",
    "#                                    beta=0.25, remap=remap, sane_index_shape=sane_index_shape)\n",
    "#         quant_conv = nn.Conv(embed_dim, (1, 1))\n",
    "#         post_quant_conv = nn.Conv(ddconfig[\"z_channels\"], (1, 1))\n",
    "\n",
    "#         # What is this??\n",
    "#         if colorize_nlabels is not None:\n",
    "#             colorize = self.param('colorize', nn.initializers.normal(), (3, self.colorize_nlabels, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "20e8a3ef-e686-4e20-a8fa-05880a0ea4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.4476189613342285\n"
     ]
    }
   ],
   "source": [
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    latent_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.dense1 = nn.Dense(features=128)\n",
    "        self.dense2 = nn.Dense(features=64)\n",
    "        self.mean_dense = nn.Dense(features=self.latent_dim)\n",
    "        self.logvar_dense = nn.Dense(features=self.latent_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = nn.relu(self.dense1(x))\n",
    "        x = nn.relu(self.dense2(x))\n",
    "        mean = self.mean_dense(x)\n",
    "        logvar = self.logvar_dense(x)\n",
    "        return mean, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    output_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.dense1 = nn.Dense(features=64)\n",
    "        self.dense2 = nn.Dense(features=128)\n",
    "        self.output_dense = nn.Dense(features=self.output_dim)\n",
    "\n",
    "    def __call__(self, z):\n",
    "        z = nn.relu(self.dense1(z))\n",
    "        z = nn.relu(self.dense2(z))\n",
    "        z = nn.sigmoid(self.output_dense(z))\n",
    "        return z\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    latent_dim: int\n",
    "    output_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.encoder = Encoder(latent_dim=self.latent_dim)\n",
    "        self.decoder = Decoder(output_dim=self.output_dim)\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = jnp.exp(0.5 * logvar)\n",
    "        eps = jax.random.normal(self.make_rng('dropout'), mean.shape)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def __call__(self, x, deterministic=True):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        reconstructed_x = self.decoder(z)\n",
    "        return reconstructed_x, mean, logvar\n",
    "\n",
    "# Example usage\n",
    "vae = VAE(latent_dim=10, output_dim=784)\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "dummy_input = jnp.ones((1, 784))  # Example input\n",
    "params = vae.init(rng, dummy_input)\n",
    "\n",
    "# Define forward pass function\n",
    "def forward_pass(params, x, rng):\n",
    "    return vae.apply(params, x, rngs={'dropout': rng})\n",
    "\n",
    "# Example forward pass\n",
    "rng = jax.random.PRNGKey(1)\n",
    "x = jax.random.normal(rng, (1, 784))\n",
    "reconstructed_x, mean, logvar = forward_pass(params, x, rng)\n",
    "\n",
    "# Training step function\n",
    "@jax.jit\n",
    "def train_step(state, batch, rng):\n",
    "    def loss_fn(params):\n",
    "        reconstructed_x, mean, logvar = forward_pass(params, batch, rng)\n",
    "        reconstruction_loss = jnp.mean(jnp.square(reconstructed_x - batch))\n",
    "        kl_divergence = -0.5 * jnp.mean(1 + logvar - jnp.square(mean) - jnp.exp(logvar))\n",
    "        loss = reconstruction_loss + kl_divergence\n",
    "        return loss\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "# Initialize optimizer and training state\n",
    "optimizer = optax.adam(learning_rate=1e-3)\n",
    "state = train_state.TrainState.create(apply_fn=vae.apply, params=params, tx=optimizer)\n",
    "\n",
    "# Example training step\n",
    "rng, step_rng = jax.random.split(rng)\n",
    "state, loss = train_step(state, x, step_rng)\n",
    "\n",
    "print(f\"Training loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d5aac871-bffe-4ad1-a75d-28125eef0fbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[0.59782356, 0.48028037, 0.5268784 , 0.5412499 , 0.47357202,\n",
       "         0.5295619 , 0.55362946, 0.40527472, 0.41515607, 0.47278085,\n",
       "         0.3736489 , 0.6215862 , 0.4283234 , 0.5297417 , 0.47881293,\n",
       "         0.5251219 , 0.5943308 , 0.56700146, 0.48229563, 0.5048226 ,\n",
       "         0.3958562 , 0.60273117, 0.59007645, 0.4164364 , 0.54597217,\n",
       "         0.53243494, 0.5871151 , 0.51926935, 0.4613332 , 0.50815547,\n",
       "         0.45364735, 0.4747558 , 0.505987  , 0.5269336 , 0.38053897,\n",
       "         0.55809027, 0.44448796, 0.47760388, 0.5172707 , 0.48258492,\n",
       "         0.39854977, 0.57031465, 0.45915866, 0.5885839 , 0.5549953 ,\n",
       "         0.4484396 , 0.41627893, 0.5520951 , 0.5824886 , 0.55574363,\n",
       "         0.5655874 , 0.60568774, 0.484931  , 0.54635173, 0.5347197 ,\n",
       "         0.5201308 , 0.50564045, 0.51796824, 0.5792515 , 0.5502594 ,\n",
       "         0.59239864, 0.45128104, 0.53559744, 0.47412473, 0.5295527 ,\n",
       "         0.46331114, 0.53153205, 0.46626937, 0.48959446, 0.5379729 ,\n",
       "         0.4250202 , 0.47033706, 0.44528496, 0.52474076, 0.42995775,\n",
       "         0.47183523, 0.49665818, 0.53258264, 0.5852597 , 0.55461866,\n",
       "         0.5951631 , 0.5476991 , 0.5235748 , 0.5117078 , 0.48831666,\n",
       "         0.38791445, 0.4947308 , 0.4290914 , 0.46749586, 0.42546642,\n",
       "         0.4597331 , 0.5282411 , 0.5169636 , 0.55490583, 0.4565537 ,\n",
       "         0.57070065, 0.58015513, 0.44072992, 0.44396177, 0.53144777,\n",
       "         0.44292435, 0.53856957, 0.5539565 , 0.4215931 , 0.507648  ,\n",
       "         0.57745343, 0.37751627, 0.54032815, 0.37829813, 0.5546306 ,\n",
       "         0.5231067 , 0.52409476, 0.43869   , 0.516181  , 0.5455535 ,\n",
       "         0.37391815, 0.4402671 , 0.44863963, 0.45839974, 0.54731226,\n",
       "         0.5376852 , 0.4618504 , 0.45604962, 0.47464424, 0.71234566,\n",
       "         0.4899607 , 0.48090667, 0.524105  , 0.59175307, 0.41598356,\n",
       "         0.51977605, 0.48085126, 0.48214036, 0.48620892, 0.39989528,\n",
       "         0.47117695, 0.48021224, 0.48533827, 0.4773453 , 0.5622322 ,\n",
       "         0.41939402, 0.49476334, 0.5211893 , 0.5578551 , 0.4380661 ,\n",
       "         0.52621406, 0.5329886 , 0.5048535 , 0.50178564, 0.5270206 ,\n",
       "         0.47451973, 0.43505734, 0.5046626 , 0.5459244 , 0.5470197 ,\n",
       "         0.48845434, 0.54756   , 0.5507845 , 0.45138153, 0.5058057 ,\n",
       "         0.5826401 , 0.5417129 , 0.5049236 , 0.40098384, 0.5201543 ,\n",
       "         0.4818125 , 0.6270194 , 0.53907436, 0.62454367, 0.4982315 ,\n",
       "         0.5774093 , 0.60415286, 0.49643573, 0.5659928 , 0.4669579 ,\n",
       "         0.55662197, 0.5643804 , 0.4285826 , 0.48653513, 0.485261  ,\n",
       "         0.46204564, 0.45391303, 0.4861816 , 0.6061903 , 0.56409746,\n",
       "         0.54096377, 0.48349828, 0.3962917 , 0.4993035 , 0.46921816,\n",
       "         0.50872654, 0.61344063, 0.48021883, 0.36001208, 0.56161153,\n",
       "         0.48162952, 0.44948378, 0.52637416, 0.5049854 , 0.41589713,\n",
       "         0.49804205, 0.5330482 , 0.49233127, 0.4513726 , 0.59796417,\n",
       "         0.55704665, 0.5259357 , 0.48111314, 0.36744365, 0.4492578 ,\n",
       "         0.5056892 , 0.5048241 , 0.5201272 , 0.6014734 , 0.5033623 ,\n",
       "         0.44540074, 0.5299907 , 0.6071762 , 0.52867216, 0.46978194,\n",
       "         0.47186658, 0.41519988, 0.4781695 , 0.59361106, 0.5340304 ,\n",
       "         0.50490296, 0.4584254 , 0.47369462, 0.5308742 , 0.4565056 ,\n",
       "         0.48547623, 0.59674853, 0.5212606 , 0.5015076 , 0.5072645 ,\n",
       "         0.47868302, 0.4253915 , 0.48044956, 0.4640237 , 0.59059614,\n",
       "         0.52121836, 0.5031649 , 0.6008597 , 0.42704362, 0.47119617,\n",
       "         0.5453722 , 0.46283844, 0.48945343, 0.5099894 , 0.40942886,\n",
       "         0.45028162, 0.45361203, 0.5613183 , 0.4041763 , 0.49939063,\n",
       "         0.63314575, 0.4711948 , 0.47820318, 0.451949  , 0.5427748 ,\n",
       "         0.4262356 , 0.4425754 , 0.5362935 , 0.50969   , 0.50896496,\n",
       "         0.5122675 , 0.6492443 , 0.5213038 , 0.4421406 , 0.5659144 ,\n",
       "         0.54433256, 0.39421976, 0.45502353, 0.5378962 , 0.58835876,\n",
       "         0.5623452 , 0.41843194, 0.53370786, 0.50074166, 0.5042827 ,\n",
       "         0.4731488 , 0.47923434, 0.51947737, 0.37868613, 0.43705916,\n",
       "         0.539683  , 0.5541545 , 0.4643615 , 0.5863913 , 0.4583483 ,\n",
       "         0.41794276, 0.478876  , 0.5049384 , 0.46146742, 0.54139006,\n",
       "         0.41174442, 0.4801237 , 0.4375288 , 0.5276811 , 0.5479263 ,\n",
       "         0.48213163, 0.5227895 , 0.39922225, 0.4338258 , 0.53611755,\n",
       "         0.5115464 , 0.5302944 , 0.50284135, 0.4490776 , 0.48938677,\n",
       "         0.46773762, 0.38251516, 0.49155948, 0.5223498 , 0.5048929 ,\n",
       "         0.49635857, 0.49876952, 0.5561676 , 0.4633168 , 0.43853828,\n",
       "         0.4824887 , 0.56686664, 0.5152705 , 0.47558138, 0.47987866,\n",
       "         0.48924473, 0.60287684, 0.4550587 , 0.516049  , 0.51887083,\n",
       "         0.5005121 , 0.4688654 , 0.4659146 , 0.50045645, 0.42761758,\n",
       "         0.5248753 , 0.5438127 , 0.48725528, 0.562661  , 0.56456393,\n",
       "         0.60998183, 0.48642814, 0.44002137, 0.48636004, 0.58592135,\n",
       "         0.5769428 , 0.58470887, 0.5174916 , 0.5282091 , 0.43291312,\n",
       "         0.49700186, 0.5009948 , 0.46223786, 0.47150594, 0.51705676,\n",
       "         0.46790624, 0.41527838, 0.5694577 , 0.58696586, 0.5169578 ,\n",
       "         0.59019345, 0.5671023 , 0.42479557, 0.44229195, 0.513083  ,\n",
       "         0.50211924, 0.42066905, 0.5875068 , 0.5219263 , 0.47905493,\n",
       "         0.44775692, 0.53548086, 0.47041345, 0.5204849 , 0.5314695 ,\n",
       "         0.5293162 , 0.5401103 , 0.44134158, 0.49851596, 0.48903915,\n",
       "         0.57850504, 0.5694844 , 0.44248378, 0.57973933, 0.5107718 ,\n",
       "         0.48277748, 0.32096025, 0.47127223, 0.48061544, 0.41147387,\n",
       "         0.47867635, 0.4362605 , 0.54900396, 0.56264126, 0.42888546,\n",
       "         0.5321829 , 0.6088496 , 0.524403  , 0.4416403 , 0.42872   ,\n",
       "         0.57205796, 0.31987077, 0.44024315, 0.50635725, 0.48067933,\n",
       "         0.54466945, 0.45696002, 0.5396387 , 0.4651056 , 0.39447016,\n",
       "         0.49051163, 0.51861894, 0.45687237, 0.43168366, 0.47275314,\n",
       "         0.505826  , 0.6121318 , 0.4341402 , 0.48727235, 0.5388092 ,\n",
       "         0.38299915, 0.4051901 , 0.5822187 , 0.49616557, 0.4178715 ,\n",
       "         0.52179724, 0.43421286, 0.50892884, 0.44742298, 0.54153347,\n",
       "         0.59182113, 0.48307315, 0.56640655, 0.61031735, 0.5352278 ,\n",
       "         0.52068377, 0.4808567 , 0.49103174, 0.51490647, 0.45772386,\n",
       "         0.64259034, 0.46386743, 0.4966332 , 0.5419006 , 0.49559748,\n",
       "         0.5324073 , 0.547365  , 0.48449335, 0.48146152, 0.5835738 ,\n",
       "         0.38671663, 0.54231864, 0.47705534, 0.46057332, 0.4765876 ,\n",
       "         0.4687359 , 0.5299549 , 0.48264876, 0.52333677, 0.6613882 ,\n",
       "         0.44603407, 0.5675332 , 0.45756724, 0.5051706 , 0.43008763,\n",
       "         0.495988  , 0.5524024 , 0.48935676, 0.50612825, 0.5380399 ,\n",
       "         0.4890851 , 0.48276645, 0.44985205, 0.49745938, 0.4625007 ,\n",
       "         0.41410372, 0.47519985, 0.5141818 , 0.5802213 , 0.56862354,\n",
       "         0.59389865, 0.4654033 , 0.4866211 , 0.4376723 , 0.53675693,\n",
       "         0.5838027 , 0.5005901 , 0.4387242 , 0.34052864, 0.55623686,\n",
       "         0.47172517, 0.55874807, 0.4658818 , 0.5340581 , 0.4323777 ,\n",
       "         0.52933717, 0.4591887 , 0.5663245 , 0.5025555 , 0.46729133,\n",
       "         0.46725294, 0.46883607, 0.43242836, 0.57648385, 0.5827214 ,\n",
       "         0.54696876, 0.48185682, 0.43388143, 0.4196443 , 0.619354  ,\n",
       "         0.5123604 , 0.4569343 , 0.5101087 , 0.63454145, 0.50939673,\n",
       "         0.58297867, 0.48191214, 0.649867  , 0.5235729 , 0.60826135,\n",
       "         0.5190214 , 0.51337886, 0.5214059 , 0.40982103, 0.47870073,\n",
       "         0.4628214 , 0.55398273, 0.4496093 , 0.5684734 , 0.4472622 ,\n",
       "         0.34931055, 0.49821565, 0.5031247 , 0.53155893, 0.46043608,\n",
       "         0.49211508, 0.40769047, 0.49294162, 0.4572225 , 0.44580382,\n",
       "         0.57241535, 0.5772287 , 0.57767844, 0.39297736, 0.52060443,\n",
       "         0.46675622, 0.453725  , 0.50833505, 0.41159186, 0.5458039 ,\n",
       "         0.62160087, 0.52145267, 0.3358214 , 0.43914512, 0.4567669 ,\n",
       "         0.5540962 , 0.5014848 , 0.40701312, 0.51698655, 0.42367375,\n",
       "         0.5796417 , 0.44149548, 0.46022034, 0.5475982 , 0.48757192,\n",
       "         0.48377934, 0.40706986, 0.4456415 , 0.53734356, 0.5248918 ,\n",
       "         0.43713966, 0.62005466, 0.553583  , 0.40536928, 0.5837627 ,\n",
       "         0.50067216, 0.62676996, 0.5129893 , 0.47617376, 0.53436923,\n",
       "         0.44536904, 0.51668125, 0.5540305 , 0.5168143 , 0.5343007 ,\n",
       "         0.55251664, 0.5059075 , 0.48332   , 0.4636921 , 0.5346317 ,\n",
       "         0.60570306, 0.43856514, 0.51942855, 0.4720008 , 0.60657424,\n",
       "         0.5280086 , 0.5227982 , 0.5231095 , 0.528403  , 0.46927345,\n",
       "         0.49581262, 0.56621724, 0.5154653 , 0.52379924, 0.46903133,\n",
       "         0.553354  , 0.57185686, 0.5534384 , 0.5191285 , 0.5627809 ,\n",
       "         0.47062656, 0.57380086, 0.5063973 , 0.44583774, 0.535154  ,\n",
       "         0.4379747 , 0.5669688 , 0.46039823, 0.45309412, 0.5035037 ,\n",
       "         0.58647215, 0.45156056, 0.424948  , 0.49223158, 0.48381907,\n",
       "         0.529802  , 0.49314398, 0.4793951 , 0.49561962, 0.49647462,\n",
       "         0.5298759 , 0.46220896, 0.59238565, 0.58968693, 0.6017428 ,\n",
       "         0.5265557 , 0.48573586, 0.5982282 , 0.59966904, 0.5258962 ,\n",
       "         0.48833424, 0.49000916, 0.48714852, 0.5356668 , 0.41893855,\n",
       "         0.40844175, 0.58501035, 0.56449324, 0.5338009 , 0.5420297 ,\n",
       "         0.5073178 , 0.45203182, 0.48135144, 0.4820005 , 0.5433399 ,\n",
       "         0.55365384, 0.40205985, 0.5427197 , 0.5018003 , 0.56934196,\n",
       "         0.5997793 , 0.4824757 , 0.5046004 , 0.48591572, 0.49134985,\n",
       "         0.49431634, 0.5053369 , 0.5095073 , 0.5075075 , 0.40121108,\n",
       "         0.5077983 , 0.52792984, 0.555071  , 0.5059171 , 0.5320958 ,\n",
       "         0.50310403, 0.44925058, 0.5734811 , 0.43091568, 0.49927613,\n",
       "         0.50096476, 0.48346627, 0.40707397, 0.59129936, 0.46525973,\n",
       "         0.42511672, 0.6383485 , 0.4489793 , 0.6114826 , 0.58529913,\n",
       "         0.4716801 , 0.58592236, 0.40297577, 0.5321786 , 0.5164089 ,\n",
       "         0.5705215 , 0.42614543, 0.5178189 , 0.49767697, 0.42626366,\n",
       "         0.400357  , 0.46428666, 0.51387376, 0.4913023 , 0.40175286,\n",
       "         0.44235528, 0.5507951 , 0.5224197 , 0.48068252, 0.5643721 ,\n",
       "         0.47416472, 0.5744054 , 0.41211674, 0.5159561 , 0.4269051 ,\n",
       "         0.33723664, 0.48303103, 0.44524843, 0.5506025 , 0.51337117,\n",
       "         0.47436914, 0.4999956 , 0.56055087, 0.5204488 , 0.39256608,\n",
       "         0.5428425 , 0.55342835, 0.5463903 , 0.4547453 , 0.6469696 ,\n",
       "         0.5321157 , 0.4425717 , 0.4972735 , 0.5009342 , 0.45894933,\n",
       "         0.5464717 , 0.56532156, 0.5468966 , 0.49612352, 0.58687115,\n",
       "         0.53652245, 0.46478847, 0.46204838, 0.5502191 , 0.4616048 ,\n",
       "         0.43002465, 0.46692905, 0.54252946, 0.5281797 , 0.48919427,\n",
       "         0.532745  , 0.45255342, 0.42416045, 0.5474619 , 0.5248538 ,\n",
       "         0.51587963, 0.36564228, 0.41390616, 0.5362583 , 0.4480377 ,\n",
       "         0.50699174, 0.56623507, 0.39608386, 0.5198998 , 0.61333495,\n",
       "         0.5545973 , 0.53066534, 0.4630832 , 0.53478366, 0.5732943 ,\n",
       "         0.48682827, 0.47309935, 0.44957897, 0.45223406, 0.49281555,\n",
       "         0.50047934, 0.5737091 , 0.46562496, 0.62386465, 0.47484875,\n",
       "         0.54249465, 0.4390324 , 0.3643042 , 0.41292077]], dtype=float32),\n",
       " Array([[ 0.0339888 ,  0.25236106, -0.0851688 ,  0.11212602, -0.60702693,\n",
       "          0.00289916, -0.09746519, -0.14254496, -0.06265313,  0.2791397 ]],      dtype=float32),\n",
       " Array([[ 0.20330687,  0.28385863,  0.07594076, -0.03722425,  0.2633126 ,\n",
       "         -0.00755761, -0.38492024, -0.14915112, -0.10575105, -0.05497728]],      dtype=float32))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.apply(state.params, x, rngs={'dropout': rng})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b280bbb-b4d2-416e-9abb-cf8c23d89a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10248458  0.2624932   0.33315787 -0.03567007  0.9329739 ]]\n",
      "[[ 0.10248458  0.2624932   0.33315787 -0.03567007  0.9329739 ]]\n"
     ]
    }
   ],
   "source": [
    "from flax import linen as nn\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    hidden_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.dense = nn.Dense(features=self.hidden_dim)\n",
    "        # Define a parameter named 'bias' with a custom initialization function\n",
    "        self.bias = self.param('bias', nn.initializers.zeros, (self.hidden_dim,))\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # Apply the dense layer and add the bias parameter\n",
    "        return self.compute_with_bias(x)\n",
    "    \n",
    "    def compute_with_bias(self, x):\n",
    "        # Access the bias parameter defined in setup\n",
    "        x = self.dense(x)\n",
    "        return x + self.bias\n",
    "\n",
    "# Example usage\n",
    "rng = jax.random.PRNGKey(0)\n",
    "dummy_input = jnp.ones((1, 10))  # Example input\n",
    "\n",
    "# Initialize the module\n",
    "module = MyModule(hidden_dim=5)\n",
    "params = module.init(rng, dummy_input)\n",
    "\n",
    "# Apply the module using the __call__ method\n",
    "output = module.apply(params, dummy_input)\n",
    "print(output)\n",
    "\n",
    "# Directly call compute_with_bias using the apply method\n",
    "output_direct = module.apply(params, dummy_input, method=MyModule.compute_with_bias)\n",
    "print(output_direct)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jax_tpu_py39)",
   "language": "python",
   "name": "jax_tpu_py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
